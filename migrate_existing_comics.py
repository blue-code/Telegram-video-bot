"""
ê¸°ì¡´ ì—…ë¡œë“œëœ ZIP íŒŒì¼ì„ ë§Œí™”ì±…ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python migrate_existing_comics.py
"""

import asyncio
import os
import sys
import logging
import base64
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.db import get_database, get_files, save_comic_metadata
from src.comic_parser import is_comic_book, extract_comic_metadata

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def migrate_comics():
    """ê¸°ì¡´ ZIP/CBZ íŒŒì¼ì„ ë§Œí™”ì±…ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""

    logger.info("=" * 60)
    logger.info("ë§Œí™”ì±… ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    logger.info("=" * 60)

    try:
        # Supabase ì—°ê²°
        sb = await get_database()
        logger.info("âœ“ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

        # ëª¨ë“  íŒŒì¼ ê°€ì ¸ì˜¤ê¸° (ëŒ€ëŸ‰ ì²˜ë¦¬)
        offset = 0
        limit = 100
        total_files = 0
        total_comics = 0
        total_skipped = 0

        while True:
            # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  ì‚¬ìš©ì)
            result = await sb.table("files").select("*").order("created_at", desc=False).range(offset, offset + limit - 1).execute()

            if not result.data:
                break

            files = result.data
            logger.info(f"\nğŸ“¦ {offset + 1}~{offset + len(files)} íŒŒì¼ ì²˜ë¦¬ ì¤‘...")

            for file_data in files:
                total_files += 1

                file_id = file_data.get("id")
                file_name = file_data.get("file_name", "")
                file_path = file_data.get("file_path")
                user_id = file_data.get("user_id")

                # ZIP ë˜ëŠ” CBZ íŒŒì¼ë§Œ ì²˜ë¦¬
                if not file_name.lower().endswith(('.zip', '.cbz')):
                    continue

                # íŒŒì¼ ê²½ë¡œ í™•ì¸
                if not file_path or not os.path.exists(file_path):
                    logger.warning(f"  âš ï¸  íŒŒì¼ ì—†ìŒ: {file_name}")
                    total_skipped += 1
                    continue

                # ì´ë¯¸ comics í…Œì´ë¸”ì— ìˆëŠ”ì§€ í™•ì¸
                existing = await sb.table("comics").select("id").eq("file_id", file_id).execute()
                if existing.data:
                    logger.debug(f"  â­ï¸  ì´ë¯¸ ë“±ë¡ë¨: {file_name}")
                    total_skipped += 1
                    continue

                # ë§Œí™”ì±…ì¸ì§€ í™•ì¸
                try:
                    is_comic = await asyncio.to_thread(is_comic_book, file_path)

                    if not is_comic:
                        logger.debug(f"  âŒ ë§Œí™”ì±… ì•„ë‹˜: {file_name}")
                        total_skipped += 1
                        continue

                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì›ë³¸ íŒŒì¼ëª… ì „ë‹¬)
                    comic_meta = await asyncio.to_thread(
                        extract_comic_metadata,
                        file_path,
                        None,  # folder
                        file_name  # original_filename
                    )

                    # Convert thumbnail bytes to base64 for JSON serialization
                    cover_bytes = comic_meta.get("cover_bytes")
                    cover_base64 = base64.b64encode(cover_bytes).decode('utf-8') if cover_bytes else None

                    # DBì— ì €ì¥
                    comic_db_data = {
                        "file_id": file_id,
                        "user_id": user_id,
                        "title": comic_meta.get("title") or file_name,
                        "series": comic_meta.get("series"),
                        "volume": comic_meta.get("volume"),
                        "folder": comic_meta.get("folder"),
                        "page_count": comic_meta.get("page_count", 0),
                        "metadata": {
                            "cover_base64": cover_base64,
                            "cover_ext": comic_meta.get("cover_ext")
                        }
                    }

                    await save_comic_metadata(comic_db_data)

                    total_comics += 1
                    logger.info(f"  âœ… ë“±ë¡ ì™„ë£Œ: {comic_meta.get('title')} (ì‹œë¦¬ì¦ˆ: {comic_meta.get('series')}, {comic_meta.get('page_count')}p)")

                except Exception as e:
                    logger.error(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {file_name} - {e}")
                    total_skipped += 1
                    continue

            # ë‹¤ìŒ ë°°ì¹˜ë¡œ ì´ë™
            offset += limit

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            logger.info(f"ì§„í–‰ ìƒí™©: ì´ {total_files}ê°œ íŒŒì¼ ê²€ì‚¬, {total_comics}ê°œ ë§Œí™”ì±… ë“±ë¡, {total_skipped}ê°œ ìŠ¤í‚µ")

        # ìµœì¢… ê²°ê³¼
        logger.info("\n" + "=" * 60)
        logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š ì´ ê²€ì‚¬ íŒŒì¼: {total_files}ê°œ")
        logger.info(f"âœ… ë“±ë¡ëœ ë§Œí™”ì±…: {total_comics}ê°œ")
        logger.info(f"â­ï¸  ìŠ¤í‚µëœ íŒŒì¼: {total_skipped}ê°œ")
        logger.info("=" * 60)

        if total_comics > 0:
            logger.info("\nğŸ‰ ë§Œí™”ì±… ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í™•ì¸í•˜ì„¸ìš”!")
            logger.info("   http://localhost:8000/comics/{user_id}")

    except Exception as e:
        logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸ“š ë§Œí™”ì±… ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬")
    print("=" * 60)
    print("ê¸°ì¡´ ì—…ë¡œë“œëœ ZIP/CBZ íŒŒì¼ì„ ë§Œí™”ì±…ìœ¼ë¡œ ë“±ë¡í•©ë‹ˆë‹¤.\n")

    response = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")

    if response.lower() != 'y':
        print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)

    print()
    asyncio.run(migrate_comics())
